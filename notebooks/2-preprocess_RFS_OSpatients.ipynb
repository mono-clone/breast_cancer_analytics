{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb93b3cc-7680-4f98-812e-608d9bcfda32",
   "metadata": {},
   "source": [
    "# preprocess of recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d992235-d9ef-4a88-a47e-2ed687ece7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なライブラリ\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import (\n",
    "    GenericUnivariateSelect,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    chi2,\n",
    ")\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# https://github.com/smazzanti/mrmr\n",
    "# pipでinstallはできたが、そのままimportできなかったので、\n",
    "# ライブラリのソースコードをそのまま環境に設置\n",
    "from libraries.mrmr import mrmr\n",
    "\n",
    "import config\n",
    "import functions\n",
    "\n",
    "SEED = config.SEED\n",
    "THRESHOLD_YEARS = config.THRESHOLD_YEARS\n",
    "THRESHOLD_MONTHS = config.THRESHOLD_MONTHS\n",
    "TARGET_NAME = \"RFS_OVER_{0}MONTHS\".format(THRESHOLD_MONTHS)\n",
    "functions.fix_seed(SEED)\n",
    "\n",
    "\n",
    "# 最大表示列数の指定（ここでは50列を指定）N\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8994dbf-0e36-421e-aed3-8cb4e247aded",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af54eb16-b22c-4ec8-b2e2-bd7ce542e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR + \"/data_clinical_patient.txt\", header=4\n",
    ")\n",
    "df_sample = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR + \"/data_clinical_sample.txt\", header=4\n",
    ")\n",
    "df_clinical = pd.merge(df_patient, df_sample, on=\"PATIENT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7f963-665a-42c0-a5e0-867c83be3490",
   "metadata": {
    "tags": []
   },
   "source": [
    "## カラムの順序変更（読みやすさのため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d8c6c4-0913-4782-beb2-2efb2b7b6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_columns(df: pd.DataFrame, regex: str):\n",
    "    # まとめたいcolumnの正規表現を一時退避\n",
    "    _df = df.copy()\n",
    "    df_tmp = _df.filter(regex=regex)\n",
    "    # 元のdfから落とす\n",
    "    _df.drop(df_tmp.columns, axis=1, inplace=True)\n",
    "    # 元のdfに結合\n",
    "    return pd.merge(_df, df_tmp, right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "def sort_columns_by_knowledge(df):\n",
    "    _df = df.copy()\n",
    "    # 癌の種類\n",
    "    _df = align_columns(_df, \"^CANCER_\")\n",
    "    # 重要そう（直感）な特徴量\n",
    "    _df = align_columns(_df, \"^ER_|^HER2_|^TUMOR_\")\n",
    "    # 治療の種類\n",
    "    _df = align_columns(_df, \".*THERAPY$|^BREAST_SURGERY\")\n",
    "    # target系の種類（OS, RFS, VITAL）\n",
    "    _df = align_columns(_df, \"^OS_.*|^RFS_.*|^VITAL_.*\")\n",
    "    return _df\n",
    "\n",
    "\n",
    "df_clinical = sort_columns_by_knowledge(df_clinical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c8d29b-a433-4083-bb05-6f1f277181cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985, 35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(524, 35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データを大きく2つに分割できるので、ここで分割\n",
    "df_MB = df_clinical[df_clinical[\"PATIENT_ID\"].str.contains(\"MB\")]\n",
    "df_MTST = df_clinical[df_clinical[\"PATIENT_ID\"].str.contains(\"MTS-T\")]\n",
    "\n",
    "df_MB.set_index(\"PATIENT_ID\", inplace=True)\n",
    "df_MTST.set_index(\"PATIENT_ID\", inplace=True)\n",
    "\n",
    "display(df_MB.shape, df_MTST.shape)\n",
    "# save\n",
    "functions.make_dir(config.INTERIM_PREPROCESSED_RECURRENCE_DIR)\n",
    "df_clinical.to_pickle(config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_clinical.pkl\")\n",
    "df_MB.to_pickle(config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_MB.pkl\")\n",
    "df_MTST.to_pickle(config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_MTST.pkl\")\n",
    "del df_patient, df_sample, df_clinical, df_MB, df_MTST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bca36f-6dc2-4ff9-9da8-8c819d4fb638",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 臨床データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a32df6-05d6-4499-9064-d1c1c263a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MB = pd.read_pickle(config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_MB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1d298-2049-4fa6-aa8b-d355287ff3e4",
   "metadata": {},
   "source": [
    "## 遺伝子データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67529b1e-6763-4afd-984e-07c3e825583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遺伝子発現データ\n",
    "# 生の遺伝子発現データ\n",
    "df_mrna_agilent_microarray = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR + \"/data_mrna_agilent_microarray.txt\", index_col=0\n",
    ").T\n",
    "df_mrna_agilent_microarray = df_mrna_agilent_microarray.drop(\n",
    "    \"Entrez_Gene_Id\"\n",
    ").sort_index()\n",
    "# zスコア化済み\n",
    "df_mrna_agilent_microarray_zscores_ref_all_samples = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR\n",
    "    + \"/data_mrna_agilent_microarray_zscores_ref_all_samples.txt\",\n",
    "    index_col=0,\n",
    ").T\n",
    "df_mrna_agilent_microarray_zscores_ref_all_samples = (\n",
    "    df_mrna_agilent_microarray_zscores_ref_all_samples.drop(\"Entrez_Gene_Id\")\n",
    ").sort_index()\n",
    "\"\"\"\n",
    "# zスコア化（2倍体基準）済み\n",
    "df_mrna_agilent_microarray_zscores_ref_diploid_samples = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR\n",
    "    + \"/data_mrna_agilent_microarray_zscores_ref_diploid_samples.txt\",\n",
    "    index_col=0,\n",
    ").T\n",
    "df_mrna_agilent_microarray_zscores_ref_diploid_samples = (\n",
    "    df_mrna_agilent_microarray_zscores_ref_diploid_samples.drop(\"Entrez_Gene_Id\")\n",
    ").sort_index()\n",
    "\n",
    "# cnaデータ\n",
    "df_cna = pd.read_table(config.RAW_BRCA_METABRIC_DIR + \"/data_cna.txt\", index_col=0).T\n",
    "df_cna = df_cna.drop(df_cna.index[0])\n",
    "\n",
    "df_methylation_promoters_rrbs = pd.read_table(\n",
    "    config.RAW_BRCA_METABRIC_DIR + \"/data_methylation_promoters_rrbs.txt\", index_col=0\n",
    ").T.sort_index()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b5ac2-d843-4e66-99ff-947f3822a874",
   "metadata": {},
   "source": [
    "# 前処理\n",
    "- 目的変数生成\n",
    "- 特徴量生成\n",
    "- 特徴名のrename \n",
    "- 欠損値の削除\n",
    "\n",
    "\n",
    "## 目的変数の生成\n",
    "\n",
    "モデルに入力する段階の仮定として、再発は既に判明している（データ取得時にはすでに手術を終えている状況であり、再発の有無についても見ているため）。\n",
    "そこで、【再発年数が**n年**以内か、以後か】に注目し、RFS_MONTHSから目的変数を生成する。    \n",
    "RFS_MONTHSは非再発者の最終フォローアップまでの月数も記録されているが、そちらはデータフレームを操作するときにRFS_STATUSからフィルタリングする。\n",
    "\n",
    "## 特徴量生成\n",
    "予測の**層別化に必要な特徴を生成**する。\n",
    "\n",
    "**層別化に必要な特徴**\n",
    "- CLAUDIN_SUBTYPE\n",
    "- NPI\n",
    "- TUMOR_SIZE\n",
    "- LYMPH_NODES_EXAMINED_POSITIVE\n",
    "\n",
    "数値データは層別化のためにカテゴリ化を行う。\n",
    "各カテゴリ化の根拠は以下の通り。\n",
    "- NPI：( ノッティンガムの予後指数：https://en.wikipedia.org/wiki/Nottingham_Prognostic_Index )\n",
    "- TUMOR_SIZE：乳癌のステージの定義( https://oshiete-gan.jp/breast/diagnosis/stages/detail.html )\n",
    "- LYMPH_NODES_EXAMINED_POSITIVE：https://medical.nikkeibp.co.jp/leaf/all/cancernavi/news/201403/535575.html\n",
    "\n",
    "## 特徴量名のrename\n",
    "重複した特徴量は別のものとして扱う\n",
    "\n",
    "\n",
    "## 欠損値の削除\n",
    "- 欠損値が多い→特徴量の削除  \n",
    "- 欠損値が少ない→サンプルの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f54d52-bf1a-41da-a455-2ff21eda8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target(target_months: int = THRESHOLD_MONTHS):\n",
    "    # RFS_STATUSのみにnullがあるため、そのデータについては患者データを削除する\n",
    "    df_MB.dropna(subset=\"RFS_STATUS\", inplace=True)\n",
    "    # 予測ラベルを扱いやすい形に変更\n",
    "    df_MB[\"RFS_STATUS\"] = df_MB[\"RFS_STATUS\"].replace(\n",
    "        {\"1:Recurred\": 1, \"0:Not Recurred\": 0}\n",
    "    )\n",
    "    df_MB[TARGET_NAME] = pd.cut(\n",
    "        df_MB[\"RFS_MONTHS\"], [0, target_months, np.inf], labels=[0, 1]\n",
    "    )\n",
    "    df_MB.dropna(inplace=True, subset=TARGET_NAME)\n",
    "\n",
    "\n",
    "def generate_features():\n",
    "    df_MB[\"NPI_CAT\"] = pd.cut(\n",
    "        df_MB.NPI,\n",
    "        [0, 2.0, 2.4, 3.4, 5.4, np.inf],\n",
    "        labels=[\"0.0~2.0\", \"2.0~2.4\", \"2.4~3.4\", \"3.4~5.4\", \"5.4~inf\"],\n",
    "    )\n",
    "    df_MB[\"TUMOR_CAT\"] = pd.cut(\n",
    "        df_MB.TUMOR_SIZE, [0, 20, 50, np.inf], labels=[\"0~20\", \"20~50\", \"50~inf\"]\n",
    "    )\n",
    "    df_MB[\"LYMPH_CAT\"] = pd.cut(\n",
    "        df_MB.LYMPH_NODES_EXAMINED_POSITIVE,\n",
    "        [-np.inf, 0, 3, np.inf],\n",
    "        labels=[\"0\", \"1~3\", \"4~inf\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def rename_duplicatged_columns(df):\n",
    "    _df = df.copy()\n",
    "    # 重複特徴量の確認\n",
    "    print(\n",
    "        \"重複特徴名数（rename前）：\",\n",
    "        _df.columns[_df.columns.duplicated()].value_counts().sum(),\n",
    "    )\n",
    "\n",
    "    cols = pd.Series(_df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [\n",
    "            dup + \"_\" + str(i) if i != 0 else dup for i in range(sum(cols == dup))\n",
    "        ]\n",
    "\n",
    "    # rename the columns with the cols list.\n",
    "    _df.columns = cols\n",
    "    # 重複特徴量の確認\n",
    "    print(\n",
    "        \"重複特徴名数（rename後）：\",\n",
    "        _df.columns[_df.columns.duplicated()].value_counts().sum(),\n",
    "    )\n",
    "    return _df\n",
    "\n",
    "\n",
    "def drop_null4cols(df):\n",
    "    _df = df.copy()\n",
    "    print(\n",
    "        \"欠損値が多い特徴個数：\",\n",
    "        (_df.isnull().sum() > _df.shape[0] // 10).sum(),\n",
    "    )\n",
    "    features = _df.isnull().sum().sort_values()[::-1]\n",
    "    features.plot()\n",
    "\n",
    "    # 多数の欠損値を持つ特徴\n",
    "    many_null_features = features[_df.isnull().sum() > _df.shape[0] // 10].index\n",
    "    # 多数の欠損値を持つ特徴の削除\n",
    "    _df.drop(many_null_features, axis=1, inplace=True)\n",
    "    return _df\n",
    "\n",
    "\n",
    "def drop_null4raws(df):\n",
    "    _df = df.copy()\n",
    "    print(\n",
    "        \"欠損値が少ない特徴個数：\",\n",
    "        ((_df.isnull().sum() < +_df.shape[0] // 10) & (_df.isnull().sum() > 0)).sum(),\n",
    "    )\n",
    "    features = _df.isnull().sum().sort_values()[::-1]\n",
    "    features.plot()\n",
    "\n",
    "    # 少数の欠損値を持つ特徴\n",
    "    few_null_features = features[\n",
    "        (_df.isnull().sum() <= _df.shape[0] // 10) & (_df.isnull().sum() > 0)\n",
    "    ]\n",
    "    # 少数の欠損値の遺伝子発現を持つ患者ID\n",
    "    list_patient_id_contains_null_expressions = list()\n",
    "    for name in few_null_features.index:\n",
    "        for patient_id in _df[_df[name].isnull()].index:\n",
    "            list_patient_id_contains_null_expressions.append(patient_id)\n",
    "\n",
    "    # 少数の欠損値の遺伝子発現を持つ患者の削除\n",
    "    _df.drop(list_patient_id_contains_null_expressions, inplace=True)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c265983-f227-4faa-86d5-25ac6c2a0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 臨床データ\n",
    "generate_target()\n",
    "generate_features()\n",
    "# 遺伝子データ\n",
    "df_gene_expressions = df_mrna_agilent_microarray.copy()\n",
    "df_gene_expressions = rename_duplicatged_columns(df_gene_expressions)\n",
    "df_gene_expressions = drop_null4cols(df_gene_expressions)\n",
    "df_gene_expressions = drop_null4raws(df_gene_expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246efeb-3fdd-4192-95a3-b1a5ef3bbd0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 臨床データと遺伝子データの結合\n",
    "\n",
    "患者の必要な特徴を含む臨床データと遺伝子データを結合する。\n",
    "\n",
    "## 臨床データ\n",
    "\n",
    "**必要な特徴**\n",
    "\n",
    "目的変数\n",
    "- RFS_OVER_nMONTHS  \n",
    "\n",
    "層別化対象候補\n",
    "- CLAUDIN_SUBTYPE\n",
    "- NPI_CAT\n",
    "- TUMOR_CAT\n",
    "- LYMPH_CAT\n",
    "\n",
    "## 遺伝子データ\n",
    "遺伝子データは全ての特徴量を結合する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87a9c8c-799d-492b-ad6e-3a0549d91063",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_features = [\n",
    "    \"RFS_STATUS\",\n",
    "    TARGET_NAME,\n",
    "    \"CLAUDIN_SUBTYPE\",\n",
    "    \"NPI_CAT\",\n",
    "    \"TUMOR_CAT\",\n",
    "    \"LYMPH_CAT\",\n",
    "]\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_MB[left_features],\n",
    "    df_gene_expressions,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "functions.make_dir(config.INTERIM_PREPROCESSED_RECURRENCE_DIR)\n",
    "df_gene_expressions.to_pickle(\n",
    "    config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_gene_expressions.pkl\"\n",
    ")\n",
    "df_merged.to_pickle(config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_merged.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83436e1-d418-4b14-9c73-606b393cce72",
   "metadata": {},
   "source": [
    "## 再発者の抽出\n",
    "\n",
    "再発者のみを対象とするため、再発者を抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea2a2f0-5551-442a-9516-05d9a9f2d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recurrenced = df_merged[df_merged[\"RFS_STATUS\"] == 1].drop(\"RFS_STATUS\", axis=1)\n",
    "df_recurrenced.shape\n",
    "\n",
    "# save\n",
    "functions.make_dir(config.INTERIM_PREPROCESSED_RECURRENCE_DIR)\n",
    "df_recurrenced.to_pickle(\n",
    "    config.INTERIM_PREPROCESSED_RECURRENCE_DIR + \"/df_recurrenced.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284746c-8c80-4c32-86b5-34e1d48047b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## データ分割\n",
    "\n",
    "訓練データ、検証データ、テストデータに層化分割する   \n",
    "比率は(train, val, test)=(0.81, 0.09, 0.1)  \n",
    "CVは行わずに通常のholdoutで検証は行う（特徴選択を毎度実施するのが厄介なため）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e02b5c-90c3-40a6-94d3-efecef50364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割のためのクラス\n",
    "# pythonのミュータブルオブジェクトの外部操作を防ぐためにcopy()の使用&private化\n",
    "class SplitDataFrame:\n",
    "    def __init__(self, df):\n",
    "        self.__df = df.copy()\n",
    "        self.__train_size = 0.9\n",
    "\n",
    "        self.__df_train_val = None\n",
    "        self.__df_train = None\n",
    "        self.__df_val = None\n",
    "        self.__df_test = None\n",
    "\n",
    "        self.__X_train_val = None\n",
    "        self.__y_train_val = None\n",
    "        self.__X_train = None\n",
    "        self.__y_train = None\n",
    "        self.__X_val = None\n",
    "        self.__y_val = None\n",
    "        self.__X_test = None\n",
    "        self.__y_test = None\n",
    "\n",
    "    def split_train_val_test(self):\n",
    "        # train & test\n",
    "        self.__df_train_val, self.__df_test = train_test_split(\n",
    "            self.__df,\n",
    "            train_size=self.__train_size,\n",
    "            stratify=self.__df[TARGET_NAME],\n",
    "            random_state=SEED,\n",
    "        )\n",
    "        # train & val\n",
    "        self.__df_train, self.__df_val = train_test_split(\n",
    "            self.__df_train_val,\n",
    "            train_size=self.__train_size,\n",
    "            stratify=self.__df_train_val[TARGET_NAME],\n",
    "            random_state=SEED,\n",
    "        )\n",
    "\n",
    "    def split_X_y(self):\n",
    "        self.__X_train, self.__y_train = (\n",
    "            self.__df_train.drop(TARGET_NAME, axis=1),\n",
    "            self.__df_train[TARGET_NAME],\n",
    "        )\n",
    "        self.__X_val, self.__y_val = (\n",
    "            self.__df_val.drop(TARGET_NAME, axis=1),\n",
    "            self.__df_val[TARGET_NAME],\n",
    "        )\n",
    "        self.__X_train_val, self.__y_train_val = (\n",
    "            self.__df_train_val.drop(TARGET_NAME, axis=1),\n",
    "            self.__df_train_val[TARGET_NAME],\n",
    "        )\n",
    "        self.__X_test, self.__y_test = (\n",
    "            self.__df_test.drop(TARGET_NAME, axis=1),\n",
    "            self.__df_test[TARGET_NAME],\n",
    "        )\n",
    "    \n",
    "    def get_train_val(self):\n",
    "        return self.__train_val.copy()\n",
    "\n",
    "    def get_train(self):\n",
    "        return self.__train.copy()\n",
    "    \n",
    "    def get_val(self):\n",
    "        return self.__val.copy()\n",
    "\n",
    "    def get_test(self):\n",
    "        return self.__test.copy()\n",
    "\n",
    "    def get_train_val_Xy(self):\n",
    "        return self.__X_train_val.copy(), self.__y_train_val.copy()\n",
    "\n",
    "    def get_train_Xy(self):\n",
    "        return self.__X_train.copy(), self.__y_train.copy()\n",
    "\n",
    "    def get_val_Xy(self):\n",
    "        return self.__X_val.copy(), self.__y_val.copy()\n",
    "\n",
    "    def get_test_Xy(self):\n",
    "        return self.__X_test.copy(), self.__y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949a6d0-33ef-4383-8312-949775980915",
   "metadata": {},
   "source": [
    "# 特徴選択\n",
    "\n",
    "特徴数が多いため、特徴数を削減する\n",
    "\n",
    "**目安**\n",
    "サンプル数が767件であり、8割程度が学習に使用できる(train : val : test = 0.9*0.9 : 0.9*0.1 : 0.1)ため、学習データの1割程度の60個を選択後上限数の目安とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b707ee-7c2e-48f4-b3af-fe0881c15561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 適用する処理毎にdfをまとめたclass\n",
    "class PreprocessDataFrame:\n",
    "    def __init__(self, df_raw):\n",
    "        self.__raw = df_raw.copy()\n",
    "        self.__preprocessed = None\n",
    "        self.__preprocess_methods = list()\n",
    "\n",
    "    def get_preprocessed_df(self):\n",
    "        return self.__preprocessed.copy()\n",
    "\n",
    "    def get_preprocess_methods(self):\n",
    "        return self.__preprocess_methods.copy()\n",
    "\n",
    "    # feature_selection\n",
    "    def set_vt(self, df):\n",
    "        \"\"\"\n",
    "        分散によるフィルターで特徴を選択したdf\n",
    "        \"\"\"\n",
    "        self.__preprocess_methods.append(\"vt\")\n",
    "        self.__preprocessed = df\n",
    "\n",
    "    def set_mrmr(self, df):\n",
    "        \"\"\"\n",
    "        mrmrで特徴を選択したdf\n",
    "        \"\"\"\n",
    "        self.__preprocess_methods.append(\"mrmr\")\n",
    "        self.__preprocessed = df\n",
    "\n",
    "    # scaling\n",
    "    def set_std(self, df):\n",
    "        \"\"\"\n",
    "        standarization\n",
    "        \"\"\"\n",
    "        self.__preprocess_methods.append(\"std\")\n",
    "        self.__preprocessed = df\n",
    "        # scaling\n",
    "\n",
    "    def set_norm(self, df):\n",
    "        \"\"\"\n",
    "        normalization\n",
    "        \"\"\"\n",
    "        self.__preprocess_methods.append(\"norm\")\n",
    "        self.__preprocessed = df\n",
    "\n",
    "    # sampling\n",
    "    def set_smote(self, df):\n",
    "        \"\"\"\n",
    "        smote\n",
    "        \"\"\"\n",
    "        self.__preprocess_methods.append(\"smote\")\n",
    "        self.__preprocessed = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b9e7b-a113-4df0-860e-d6044331639b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 分割データ毎に前処理を行う関数の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b06588-8a4f-490e-a67a-2b422e4aae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_df(\n",
    "    list_train: list(),\n",
    "    list_val: list(),\n",
    "    list_train_val: list(),\n",
    "    list_test: list(),\n",
    "    save_file_path: str = \".\",\n",
    "    save_file_name: str = \"sample\",\n",
    "):\n",
    "    \"\"\"\n",
    "    params\n",
    "    list_train: [X_train, y_train]\n",
    "    list_val: [X_val, y_val]\n",
    "    list_train_val: [X_train_val, y_train_val]\n",
    "    list_test: [X_val, y_val]\n",
    "    \"\"\"\n",
    "    functions.make_dir(\"{0}/train\".format(save_file_path))\n",
    "    list_train[0].to_pickle(\n",
    "        \"{0}/train/X_{1}.pkl\".format(save_file_path, save_file_name)\n",
    "    )\n",
    "    list_train[1].to_pickle(\n",
    "        \"{0}/train/y_{1}.pkl\".format(save_file_path, save_file_name)\n",
    "    )\n",
    "    functions.make_dir(\"{0}/val\".format(save_file_path))\n",
    "    list_val[0].to_pickle(\"{0}/val/X_{1}.pkl\".format(save_file_path, save_file_name))\n",
    "    list_val[1].to_pickle(\"{0}/val/y_{1}.pkl\".format(save_file_path, save_file_name))\n",
    "\n",
    "    functions.make_dir(\"{0}/train_val\".format(save_file_path))\n",
    "    list_train_val[0].to_pickle(\n",
    "        \"{0}/train_val/X_{1}.pkl\".format(save_file_path, save_file_name)\n",
    "    )\n",
    "    list_train_val[1].to_pickle(\n",
    "        \"{0}/train_val/y_{1}.pkl\".format(save_file_path, save_file_name)\n",
    "    )\n",
    "    functions.make_dir(\"{0}/test\".format(save_file_path))\n",
    "    list_test[0].to_pickle(\"{0}/test/X_{1}.pkl\".format(save_file_path, save_file_name))\n",
    "    list_test[1].to_pickle(\"{0}/test/y_{1}.pkl\".format(save_file_path, save_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825d22b-f1d1-411d-b40b-ae06fd337321",
   "metadata": {},
   "source": [
    "## scaling関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b37c628-b200-4640-b97e-f27acaad53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化を行う関数\n",
    "def transform_std(X_train: pd.DataFrame(), X_test: pd.DataFrame() = None):\n",
    "    std = StandardScaler()\n",
    "    std.fit(X_train)\n",
    "    X_train_std = pd.DataFrame(\n",
    "        std.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    "    )\n",
    "    if X_test is None:\n",
    "        return X_train_std\n",
    "    X_test_std = pd.DataFrame(\n",
    "        std.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    "    )\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "\n",
    "# 正規化を行う関数\n",
    "def transform_norm(\n",
    "    X_train: pd.DataFrame(), X_test: pd.DataFrame() = None\n",
    ") -> pd.DataFrame():\n",
    "    mm = MinMaxScaler()\n",
    "    mm.fit(X_train)\n",
    "    X_train_norm = pd.DataFrame(\n",
    "        mm.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    "    )\n",
    "    if X_test is None:\n",
    "        return X_train_norm\n",
    "    X_test_norm = pd.DataFrame(\n",
    "        mm.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    "    )\n",
    "    return X_train_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8009324f-ed10-4082-a6af-abf5cc531e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_df(\n",
    "    df: pd.DataFrame(),\n",
    "    feature_selection_method: str = None,\n",
    "    scaling_method: str = None,\n",
    "    sampling_method: str = None,\n",
    "    save_file_path: str = None,\n",
    "    save_file_name: str = None,\n",
    "    is_save: bool = False,\n",
    "):\n",
    "    _df = df.copy()\n",
    "\n",
    "    # データが少なすぎる場合は特徴選択しない\n",
    "    if _df.shape[0] < 100:\n",
    "        # print(\"data size is too small\")\n",
    "        return\n",
    "\n",
    "    # データ分割\n",
    "    sp = SplitDataFrame(_df)\n",
    "    sp.split_train_val_test()\n",
    "    sp.split_X_y()\n",
    "\n",
    "    X_train, y_train = sp.get_train_Xy()\n",
    "    X_val, y_val = sp.get_val_Xy()\n",
    "    X_train_val, y_train_val = sp.get_train_val_Xy()\n",
    "    X_test, y_test = sp.get_test_Xy()\n",
    "\n",
    "    # class初期化\n",
    "    pd_train = PreprocessDataFrame(X_train)\n",
    "    pd_val = PreprocessDataFrame(X_val)\n",
    "    pd_train_val = PreprocessDataFrame(X_train_val)\n",
    "    pd_test = PreprocessDataFrame(X_test)\n",
    "\n",
    "    # feature selection\n",
    "    if feature_selection_method == \"none\":\n",
    "        pass\n",
    "    elif feature_selection_method == \"vt\":\n",
    "        # 分散値上位の特徴を抽出\n",
    "        features = (\n",
    "            X_train.var().sort_values().tail(X_train.shape[0] // 10).index\n",
    "        )  # 学習データの1/10サイズ\n",
    "        pd_train.set_vt(X_train[features])\n",
    "        pd_val.set_vt(X_val[features])\n",
    "\n",
    "        features = (\n",
    "            X_train_val.var()\n",
    "            .sort_values()\n",
    "            .tail(X_train_val.shape[0] // 10)\n",
    "            .index  # 学習データの1/10サイズ\n",
    "        )\n",
    "        pd_train_val.set_vt(X_train_val[features])\n",
    "        pd_test.set_vt(X_test[features])\n",
    "    elif feature_selection_method == \"mrmr\":\n",
    "        features = mrmr.mrmr_classif(\n",
    "            X=X_train,            y=y_train,K=X_train.shape[0] // 10,  # 学習データの1/10サイズ\n",
    "            show_progress=False,\n",
    "        )\n",
    "        pd_train.set_mrmr(X_train[features])\n",
    "        pd_val.set_mrmr(X_val[features])\n",
    "\n",
    "        features = mrmr.mrmr_classif(\n",
    "            X=X_train_val,\n",
    "            y=y_train_val,\n",
    "            K=X_train_val.shape[0] // 10,  # 学習データの1/10サイズ\n",
    "            show_progress=False,\n",
    "        )\n",
    "        pd_train_val.set_mrmr(X_train_val[features])\n",
    "        pd_test.set_mrmr(X_test[features])\n",
    "    else:\n",
    "        print(\"undefined feature_selection_method\")\n",
    "        return\n",
    "\n",
    "    # scaling\n",
    "    if scaling_method == \"none\":\n",
    "        pass\n",
    "    elif scaling_method == \"std\":\n",
    "        X_train, X_val = transform_std(\n",
    "            pd_train.get_preprocessed_df(), pd_val.get_preprocessed_df()\n",
    "        )\n",
    "        X_train_val, X_test = transform_std(\n",
    "            pd_train_val.get_preprocessed_df(), pd_test.get_preprocessed_df()\n",
    "        )\n",
    "        pd_train.set_std(X_train)\n",
    "        pd_val.set_std(X_val)\n",
    "        pd_train_val.set_std(X_train_val)\n",
    "        pd_test.set_std(X_test)\n",
    "    elif scaling_method == \"norm\":\n",
    "        X_train, X_val = transform_norm(\n",
    "            pd_train.get_preprocessed_df(), pd_val.get_preprocessed_df()\n",
    "        )\n",
    "        X_train_val, X_test = transform_norm(\n",
    "            pd_train_val.get_preprocessed_df(), pd_test.get_preprocessed_df()\n",
    "        )\n",
    "        pd_train.set_norm(X_train)\n",
    "        pd_val.set_std(X_val)\n",
    "        pd_train_val.set_std(X_train_val)\n",
    "        pd_test.set_std(X_test)\n",
    "    else:\n",
    "        print(\"undefined scaling_method\")\n",
    "        return\n",
    "\n",
    "    # sampling\n",
    "    if sampling_method == \"none\":\n",
    "        pass\n",
    "    elif sampling_method == \"smote\":\n",
    "        smote = SMOTE(sampling_strategy=\"minority\", random_state=SEED)\n",
    "        X_train, y_train = smote.fit_resample(pd_train.get_preprocessed_df(), y_train)\n",
    "        smote = SMOTE(sampling_strategy=\"minority\", random_state=SEED)\n",
    "        X_train_val, y_train_val = smote.fit_resample(\n",
    "            pd_train_val.get_preprocessed_df(), y_train_val\n",
    "        )\n",
    "        pd_train.set_smote(X_train)\n",
    "        pd_train_val.set_smote(X_train_val)\n",
    "    else:\n",
    "        print(\"undefined sampling_method\")\n",
    "        return\n",
    "\n",
    "    # 保存\n",
    "    if is_save:\n",
    "        save_preprocessed_df(\n",
    "            list_train=[pd_train.get_preprocessed_df(), y_train],\n",
    "            list_val=[pd_val.get_preprocessed_df(), y_val],\n",
    "            list_train_val=[pd_train_val.get_preprocessed_df(), y_train_val],\n",
    "            list_test=[pd_test.get_preprocessed_df(), y_test],\n",
    "            save_file_path=save_file_path,\n",
    "            save_file_name=save_file_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fd87f4-1d1f-48aa-b328-079b0dfdcaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [1:06:35, 499.50s/it]\n"
     ]
    }
   ],
   "source": [
    "is_save = True\n",
    "\n",
    "# サブグループ削除（一時的に）\n",
    "subgroup_columns = [\n",
    "    \"CLAUDIN_SUBTYPE\",\n",
    "    \"NPI_CAT\",\n",
    "    \"TUMOR_CAT\",\n",
    "    \"LYMPH_CAT\",\n",
    "]\n",
    "\n",
    "drop_columns = [\n",
    "    \"CLAUDIN_SUBTYPE\",\n",
    "    \"NPI_CAT\",\n",
    "    \"TUMOR_CAT\",\n",
    "    \"LYMPH_CAT\",\n",
    "]\n",
    "\n",
    "feature_selection_methods = [\"vt\", \"mrmr\"]\n",
    "\n",
    "scaling_methods = [\"std\", \"norm\"]\n",
    "\n",
    "sampling_methods = [\"none\", \"smote\"]\n",
    "\n",
    "for feature_selection_method, scaling_method, sampling_method in tqdm(\n",
    "    itertools.product(feature_selection_methods, scaling_methods, sampling_methods)\n",
    "):\n",
    "    preprocess_order = \"{0}_{1}_{2}\".format(\n",
    "        feature_selection_method, scaling_method, sampling_method\n",
    "    )\n",
    "    for subgroup_column in subgroup_columns:  # 各サブグループへの適用\n",
    "        for subgroup in df_recurrenced[subgroup_column].unique():  # サブグループ毎への適用\n",
    "            df = df_recurrenced[df_recurrenced[subgroup_column] == subgroup].drop(\n",
    "                drop_columns, axis=1\n",
    "            )\n",
    "            output_file_path = \"./{0}/{1}/{2}\".format(\n",
    "                config.INTERIM_PREPROCESSED_RECURRENCE_DIR,\n",
    "                subgroup_column,\n",
    "                preprocess_order,\n",
    "            )\n",
    "            functions.make_dir(output_file_path)\n",
    "            preprocess_df(\n",
    "                df,\n",
    "                feature_selection_method=feature_selection_method,\n",
    "                scaling_method=scaling_method,\n",
    "                sampling_method=sampling_method,\n",
    "                save_file_path=output_file_path,\n",
    "                save_file_name=subgroup,\n",
    "                is_save=is_save,\n",
    "            )\n",
    "            del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5c7c0-c9df-405f-b84d-acf8af754597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
